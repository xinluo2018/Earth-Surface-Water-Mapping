{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_loader.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9Hz5b5wgdWSoDgo39O0f1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FotcwGjdg_eA","colab_type":"code","colab":{}},"source":["# mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# go to your code files directory\n","import os\n","os.chdir(\"/content/drive/My Drive/Colab/WaterMapping/Github_upload\")\n","# !ls\n","# !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8FC67JWhEOj","colab_type":"code","colab":{}},"source":["try:\n","    get_ipython().magic(u'tensorflow_version 2.x')\n","except Exception:\n","    pass\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import pathlib\n","from utils import readTiff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfAAQq97g1nc","colab_type":"code","colab":{}},"source":["###  Get the pathes (string) corresponding to image pathes, return a list\n","def get_path(folder_Scenes, folder_Truths):    \n","    path_Scenes = pathlib.Path(folder_Scenes)\n","    path_Truths = pathlib.Path(folder_Truths)\n","    Scene_paths = list(path_Scenes.glob('*'))\n","    Scene_paths = sorted([str(path) for path in Scene_paths])    \n","    Truth_paths = list(path_Truths.glob('*'))\n","    Truth_paths = sorted([str(path) for path in Truth_paths])\n","    return Scene_paths, Truth_paths\n","\n","### load the scenes\n","def load_scene(Scene_paths, Truth_paths, Patch_size):\n","    Scenes = list(range(len(Scene_paths)))   ## initialized the list\n","    Truths = list(range(len(Scene_paths)))\n","    Radios = list(range(len(Scene_paths)))  \n","    for i in range(len(Scene_paths)):\n","        Scenes[i], _, _, im_row,im_col, _ = readTiff(Scene_paths[i])\n","        Truths[i], _, _, _, _, _ = readTiff(Truth_paths[i])\n","        Truths[i] = np.expand_dims(Truths[i], axis=2)\n","        Radios[i] = (im_row//Patch_size+1)*(im_col//Patch_size+1)\n","    return Scenes, Truths, Radios\n","\n","#### Data augmentation: noisy, filp, rotate, data missing. \n","def Image_aug(scene, truth, flip = True, rot = True, noisy = True, missing = True):\n","    scene_aug = np.copy(scene)\n","    truth_aug = np.copy(truth)\n","    if noisy == True:\n","        if np.random.uniform(()) > 0.5:  \n","            noise = np.random.normal(loc = 0, scale = 0.01, size = (512, 512, scene.shape[2]))\n","            scene_aug = scene_aug + noise            \n","    if missing == True:\n","        if np.random.uniform(()) > 0.75: \n","            missing_wigth_row = random.randint(0,10)\n","            missing_wigth_col = random.randint(0,10)\n","            row_start = random.randint(0,scene_aug.shape[0]-missing_wigth_row)\n","            col_start = random.randint(0,scene_aug.shape[1]-missing_wigth_col)    \n","            scene_aug[row_start:row_start+missing_wigth_row, :, :] = 0\n","            scene_aug[:, col_start:col_start+missing_wigth_col, :] = 0\n","    if flip == True:\n","        if np.random.uniform(()) > 0.5:\n","            if random.randint(1,2) == 1:  ## horizontal or vertical mirroring\n","                scene_aug = np.flip(scene_aug, 1)\n","                truth_aug = np.flip(truth_aug, 1)\n","            else: \n","                scene_aug = np.flip(scene_aug, 0)\n","                truth_aug = np.flip(truth_aug, 0)\n","    if rot == True:\n","        if np.random.uniform(()) > 0.5:  \n","            degree = random.randint(1,3)\n","            scene_aug = np.rot90(scene_aug, k=degree)\n","            truth_aug = np.rot90(truth_aug, k=degree)\n","    return scene_aug.astype(np.float32), truth_aug.astype(np.float32)\n","\n","## crop the scenes to patches\n","def random_crop(input_scenes, real_scenes, radios, Patch_size):\n","    n_band = input_scenes[0].shape[2]\n","    PatchSet = []\n","    TruthSet = []\n","    for i in range(len(input_scenes)):    \n","        for j in range(radios[i]):\n","            random_row = random.randint(0,input_scenes[i].shape[0]-Patch_size)\n","            random_col = random.randint(0,input_scenes[i].shape[1]-Patch_size)\n","            stacked_scenes = np.concatenate([input_scenes[i], real_scenes[i]], axis=2).astype(np.float32)\n","            cropped_scenes = stacked_scenes[random_row:random_row+Patch_size, random_col:random_col+Patch_size, :]\n","            PatchSet.append(cropped_scenes[:,:,0:n_band])\n","            TruthSet.append(cropped_scenes[:,:,n_band:n_band+1])\n","    return PatchSet, TruthSet \n","\n","def get_scene(folder_Scenes, folder_Truths, PATCH_SIZE):\n","    ## input the path of the folders corresponding to scenes and truth\n","    path_Scenes, path_Truths = get_path(folder_Scenes, folder_Truths)\n","    Scenes, Truths, Ratios = load_scene(path_Scenes, path_Truths, PATCH_SIZE)\n","    Scenes = [np.clip(Scenes/10000, 0, 1) for Scenes in Scenes]  #   Normalization\n","    return Scenes, Truths\n","\n","def get_patch(Scenes, Truths, PATCH_SIZE, BATCH_SIZE, BUFFER_SIZE):\n","    ## input Scenes and Truths are the list datatype\n","    ## return tf.data.Dataset\n","    Crop_Ratios = np.ones(len(Scenes), dtype=np.int)\n","    Patches, PatchTruths = random_crop(Scenes, Truths, \n","                            radios=Crop_Ratios, Patch_size=PATCH_SIZE)\n","    # data augmentation\n","    Patches_aug = np.zeros((len(Patches), Patches[0].shape[0],\n","                                Patches[0].shape[1], Patches[0].shape[2]))\n","    PatchTruths_aug = np.zeros((len(PatchTruths), PatchTruths[0].shape[0], \n","                                PatchTruths[0].shape[1], PatchTruths[0].shape[2]))\n","    for i in range(len(Patches)):\n","        Patches_aug[i], PatchTruths_aug[i] = Image_aug(Patches[i], PatchTruths[i], \n","                            flip = True, rot = True, noisy = True, missing = False)       \n","    dataSet = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(Patches_aug), \n","                                    tf.convert_to_tensor(PatchTruths_aug)))\n","    dataSet = dataSet.batch(BATCH_SIZE).shuffle(BUFFER_SIZE)\n","    return dataSet\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSd66UWDhIzt","colab_type":"code","colab":{}},"source":["###### test the data loader functions\n","# folder_TrainScenes = '/content/drive/My Drive/Colab/WaterMapping/TrainingData/TrainingScene/' \n","# folder_TrainTruths = '/content/drive/My Drive/Colab/WaterMapping/TrainingData/TrainingTruth/'\n","# PATCH_SIZE = 512\n","# BATCH_SIZE = 4\n","# BUFFER_SIZE = 200\n","# Scenes, Truths = get_scene(folder_TrainScenes, folder_TrainTruths, PATCH_SIZE)\n","# TrainSet = get_patch(Scenes, Truths, BATCH_SIZE, BUFFER_SIZE)\n","# TrainSet\n"],"execution_count":null,"outputs":[]}]}