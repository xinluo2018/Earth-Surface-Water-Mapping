{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiZcsX8jpns5jYXxz0V5ro"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"iCMQ0mxfhIM5","colab_type":"code","colab":{}},"source":["# mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","import os\n","os.chdir(\"/content/drive/My Drive/Colab/WaterMapping/Github_upload\")\n","# !ls\n","# !nvidia-smi\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k00r6CMowgVC","colab_type":"code","colab":{}},"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","from tensorflow.keras.layers import Lambda\n","import tensorflow.keras.backend as backend\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HA99z-P26cd","colab_type":"code","colab":{}},"source":["##### MobileNetV2\n","relu6 = tf.keras.layers.ReLU(6.)\n","def _conv_block(inputs, filters, kernel, strides):\n","    x = tf.keras.layers.Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    return relu6(x)\n","def _bottleneck(inputs, filters, kernel, t, s, r=False):\n","    tchannel = inputs.shape[-1] * t\n","    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n","    x = tf.keras.layers.DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = relu6(x)\n","    x = tf.keras.layers.Conv2D(filters, (1, 1), strides=(1, 1), padding='same')(x)  # 降维，改层为瓶颈层\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    if r:\n","        x = tf.keras.layers.add([x, inputs])\n","    return x\n","\n","def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n","    x = _bottleneck(inputs, filters, kernel, t, strides)\n","    for i in range(1, n):\n","        x = _bottleneck(x, filters, kernel, t, 1, True)\n","    return x\n","\n","def MobileNetV2(input_shape, nclasses=2):\n","    \"\"\"\n","    # Arguments\n","        input_shape: An integer or tuple/list of 3 integers, shape\n","            of input tensor.\n","        classes: Integer, number of classes.\n","    # Returns\n","        MobileNetv2 model.\n","    \"\"\"\n","\n","    inputs = tf.keras.layers.Input(shape=input_shape, name='input')\n","    x = _conv_block(inputs, 32, (3, 3), strides=(2, 2))   # 0.5*size         n_layers = 1\n"," \n","    x = _inverted_residual_block(x, 16, (3, 3), t=1, strides=1, n=1)  #        n_layers = 3\n","    x = _inverted_residual_block(x, 24, (3, 3), t=6, strides=2, n=2)  # 0.5*size,  n_layers = 6\n","\n","    x = _inverted_residual_block(x, 32, (3, 3), t=6, strides=2, n=3)  # 0.5*size,  n_layers = 9\n","    x = _inverted_residual_block(x, 64, (3, 3), t=6, strides=2, n=4)  # 0.5*size,  n_layers = 12\n","    x = _inverted_residual_block(x, 96, (3, 3), t=6, strides=1, n=3)  #        n_layers = 9\n","    x = _inverted_residual_block(x, 160, (3, 3), t=6, strides=2, n=3)  # 0.5*size,  n_layers = 9\n","    x = _inverted_residual_block(x, 320, (3, 3), t=6, strides=1, n=1)  #        n_layers = 3\n","\n","    x = _conv_block(x, 1280, (1, 1), strides=(1, 1))  # n_layers = 1\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Reshape((1, 1, 1280))(x)\n","    x = tf.keras.layers.Dropout(0.3, name='Dropout')(x)\n","    x = tf.keras.layers.Conv2D(nclasses, (1, 1), padding='same')(x)  # n_layers = 1\n","    x = tf.keras.layers.Activation('softmax', name='final_activation')(x)\n","    output = tf.keras.layers.Reshape((nclasses,), name='output')(x)\n","    model = tf.keras.models.Model(inputs, output)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYz6-NNo2AX-","colab_type":"code","colab":{}},"source":["def Upsample(tensor, size_1):  \n","    '''bilinear upsampling'''\n","    y = tf.image.resize(images=tensor, size=size_1)\n","    return y\n","\n","def ASPP_2(tensor):\n","\n","    '''atrous spatial pyramid pooling'''\n","    dims = backend.int_shape(tensor)\n","    y_pool = tf.keras.layers.AveragePooling2D(pool_size=(\n","        dims[1], dims[2]), name='average_pooling')(tensor)\n","    y_pool = tf.keras.layers.Conv2D(filters=128, kernel_size=1, padding='same',\n","                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)\n","    y_pool = tf.keras.layers.BatchNormalization(name=f'bn_1')(y_pool)\n","    y_pool = tf.keras.layers.Activation('relu', name=f'relu_1')(y_pool)\n","    y_pool = Upsample(tensor=y_pool, size_1=[dims[1], dims[2]])\n","    ## 1x1 conv\n","    y_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=1, dilation_rate=1, padding='same',\n","                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)\n","    y_1 = tf.keras.layers.BatchNormalization(name=f'bn_2')(y_1)\n","    y_1 = tf.keras.layers.Activation('relu', name=f'relu_2')(y_1)\n","    ## 3x3 dilated conv\n","    y_6 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, dilation_rate=6, padding='same',\n","                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)\n","    y_6 = tf.keras.layers.BatchNormalization(name=f'bn_3')(y_6)\n","    y_6 = tf.keras.layers.Activation('relu', name=f'relu_3')(y_6)\n","    ## 3x3 dilated conv\n","    y_12 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, dilation_rate=12, padding='same',\n","                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)\n","    y_12 = tf.keras.layers.BatchNormalization(name=f'bn_4')(y_12)\n","    y_12 = tf.keras.layers.Activation('relu', name=f'relu_4')(y_12)\n","    \n","    ## 3x3 dilated conv\n","    y_18 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, dilation_rate=18, padding='same',\n","                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)\n","    y_18 = tf.keras.layers.BatchNormalization(name=f'bn_5')(y_18)\n","    y_18 = tf.keras.layers.Activation('relu', name=f'relu_5')(y_18)\n","    \n","    ## concat\n","    y = tf.keras.layers.concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')\n","    y = tf.keras.layers.Conv2D(filters=128, kernel_size=1, dilation_rate=1, padding='same',\n","               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)\n","    y = tf.keras.layers.BatchNormalization(name=f'bn_final')(y)\n","    y = tf.keras.layers.Activation('relu', name=f'relu_final')(y)\n","    return y\n","\n","def DeepLabV3Plus_improve(input_shape, base_model, d_feature, m_feature, l_feature, nclasses=2):\n","    '''\n","    Arguments:\n","        input_shape: (img_height, img_width, img_channel)\n","        base_model: backbone network\n","        d_feature, m_feature, l_feature: features corresponding \n","                    to the deep, middle, and low layers of the backbone model\n","        nclass: number of classes.\n","    '''\n","    print('*** Building DeepLabv3Plus Network ***')\n","    (img_height, img_width, img_channel) = input_shape\n","    ## deep features\n","    base_model = base_model(input_shape, nclasses)\n","    image_features = base_model.get_layer(index = d_feature).output\n","    x_a = ASPP_2(image_features) \n","    x_a = Upsample(tensor=x_a, size_1=[img_height // 4, img_width // 4])  \n","    ## middle features (1/4 patch size)\n","    x_b = base_model.get_layer(index = m_feature).output\n","    x_b = layers.Conv2D(filters=48, kernel_size=1, padding='same',\n","                 kernel_initializer='he_normal', name='low_level_projection', use_bias=False)(x_b)\n","    x_b = layers.BatchNormalization(name=f'bn_low_level_projection')(x_b)\n","    x_b = layers.Activation('relu', name='low_level_activation')(x_b)\n","    ## middle features (1/2 patch size)\n","    x_c = base_model.get_layer(index = l_feature).output\n","    x_c = layers.Conv2D(filters=48, kernel_size=1, padding='same',\n","                 kernel_initializer='he_normal', name='low_level_projection_2', use_bias=False)(x_c)\n","    x_c = layers.BatchNormalization(name=f'bn_low_level_projection_2')(x_c)\n","    x_c = layers.Activation('relu', name='low_level_activation_2')(x_c)\n","    ## concat\n","    x = layers.concatenate([x_a, x_b], name='decoder_concat_1')\n","    \n","    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu',\n","               kernel_initializer='he_normal', name='decoder_conv2d_1', use_bias=False)(x)\n","    x = layers.BatchNormalization(name=f'bn_decoder_1')(x)\n","    x = layers.Activation('relu', name='activation_decoder_1')(x)\n","    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu',\n","               kernel_initializer='he_normal', name='decoder_conv2d_2', use_bias=False)(x)\n","    x = layers.BatchNormalization(name=f'bn_decoder_2')(x)\n","    x = layers.Activation('relu', name='activation_decoder_2')(x)\n","    x = Upsample(x, [img_height//2, img_width//2])\n","    ## concat\n","    x_2 = layers.concatenate([x, x_c], name='decoder_concat_3')\n","    x_2 = layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', \n","               kernel_initializer='he_normal', name='decoder_deconv2d', use_bias=False)(x_2)\n","    x_2 = layers.BatchNormalization(name=f'bn_decoder_4')(x_2)\n","    x_2 = layers.Activation('relu', name='activation_decoder_4')(x_2)\n","    last = tf.keras.layers.Conv2D(1, (1,1),\n","                        strides=1,\n","                        padding='same',\n","                        kernel_initializer='he_normal',\n","                        activation= 'sigmoid')  ## (bs, 256, 256, 1)\n","    x_2 = last(x_2)\n","    model = models.Model(inputs=base_model.input, outputs=x_2, name='DeepLabV3_Plus_improve')\n","    print(f'*** Output_Shape => {model.output_shape} ***')\n","    return model"],"execution_count":null,"outputs":[]}]}