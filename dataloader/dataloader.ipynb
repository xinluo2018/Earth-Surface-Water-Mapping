{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataloader.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPxZ0Wwk91KPZp+SlRsg2u5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FotcwGjdg_eA","executionInfo":{"status":"ok","timestamp":1602390051649,"user_tz":-480,"elapsed":1010,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"11076877845168992135"}},"outputId":"39bdaa44-2dab-441c-ac80-3f8bb2291010","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# go to your code files directory\n","import os\n","os.chdir(\"/content/drive/My Drive/Earth-surface-water-mapping\")\n","# !ls\n","# !nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q8FC67JWhEOj","executionInfo":{"status":"ok","timestamp":1602390051651,"user_tz":-480,"elapsed":1001,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"11076877845168992135"}},"outputId":"859eab13-8f53-4603-d251-189b5e3266e0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile dataloader/dataloader.py\n","\n","try:\n","    get_ipython().magic(u'tensorflow_version 2.x')\n","except Exception:\n","    pass\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import pathlib\n","from utils.utils import readTiff\n","\n","###  Get the pathes (string) corresponding to image pathes, return a list\n","def get_path(folder_Scenes, folder_Truths):    \n","    path_Scenes = pathlib.Path(folder_Scenes)\n","    path_Truths = pathlib.Path(folder_Truths)\n","    Scene_paths = list(path_Scenes.glob('*'))\n","    Scene_paths = sorted([str(path) for path in Scene_paths])    \n","    Truth_paths = list(path_Truths.glob('*'))\n","    Truth_paths = sorted([str(path) for path in Truth_paths])\n","    return Scene_paths, Truth_paths\n","\n","### load the scenes\n","def load_scene(Scene_paths, Truth_paths, Patch_size):\n","    '''\n","    output Ratios: Scenes area/Patch area\n","    '''\n","    Scenes = list(range(len(Scene_paths)))   ## initialized the list\n","    Truths = list(range(len(Scene_paths)))\n","    Radios = list(range(len(Scene_paths)))  \n","    for i in range(len(Scene_paths)):\n","        Scenes[i], _, _, im_row,im_col, _ = readTiff(Scene_paths[i])\n","        Truths[i], _, _, _, _, _ = readTiff(Truth_paths[i])\n","        Truths[i] = np.expand_dims(Truths[i], axis=2)\n","        Radios[i] = (im_row//Patch_size+1)*(im_col//Patch_size+1)\n","    return Scenes, Truths, Radios\n","\n","#### Data augmentation: noisy, filp, rotate. \n","def image_aug(image, truth, flip = True, rot = True, noisy = True):\n","    if flip == True:\n","        if tf.random.uniform(()) > 0.5:\n","            if random.randint(1,2) == 1:  ## horizontal or vertical mirroring\n","                image = tf.image.flip_left_right(image)\n","                truth = tf.image.flip_left_right(truth)\n","            else: \n","                image = tf.image.flip_up_down(image)\n","                truth = tf.image.flip_up_down(truth)\n","    if rot == True:\n","        if tf.random.uniform(()) > 0.5: \n","            degree = random.randint(1,3)\n","            image = tf.image.rot90(image, k=degree)\n","            truth = tf.image.rot90(truth, k=degree)\n","    if noisy == True:\n","        if tf.random.uniform(()) > 0.5:\n","            std = random.uniform(0.002, 0.02)\n","            gnoise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=std, dtype=tf.float32)\n","            image = tf.add(image, gnoise)\n","    return image, truth\n","\n","def get_scene(folder_Scenes, folder_Truths, PATCH_SIZE):\n","    ## input the path of the folders corresponding to scenes and truth\n","    path_Scenes, path_Truths = get_path(folder_Scenes, folder_Truths)\n","    Scenes, Truths, Ratios = load_scene(path_Scenes, path_Truths, PATCH_SIZE)\n","    Scenes = [np.clip(Scenes/10000, 0, 1) for Scenes in Scenes]  #   Normalization\n","    return Scenes, Truths\n","\n","def get_patch(Scenes, Truths, PATCH_SIZE, BATCH_SIZE, BUFFER_SIZE):\n","    '''\n","    input: Scenes (list) and Truths (list)\n","    output: tf.data.Dataset\n","    '''\n","    # data augmentation\n","    stacked = list(zip(Scenes,Truths))\n","    stacked = [np.concatenate(imgPair,axis=2) for imgPair in stacked]\n","    stacked = [tf.convert_to_tensor(imgPair, dtype=tf.float32) for imgPair in stacked]\n","    Patches_aug = []\n","    PatchTruths_aug = []\n","    for imgPair in stacked:\n","        imgPair = tf.image.random_crop(imgPair, size=[512, 512, imgPair.shape[2]])    \n","        Patch, PatchTruth = imgPair[:,:,:Scenes[0].shape[2]], imgPair[:,:,Scenes[0].shape[2]:]\n","        Patch_aug, PatchTruth_aug = image_aug(Patch, PatchTruth,\\\n","                                 flip = True, rot = True, noisy = True)\n","        Patches_aug.append(Patch_aug)\n","        PatchTruths_aug.append(PatchTruth_aug)\n","    dataSet = tf.data.Dataset.from_tensor_slices((Patches_aug,PatchTruths_aug))\n","    dataSet = dataSet.batch(BATCH_SIZE).shuffle(BUFFER_SIZE)\n","    return dataSet\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Overwriting dataloader/dataloader.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BSd66UWDhIzt","executionInfo":{"status":"ok","timestamp":1602390068376,"user_tz":-480,"elapsed":17718,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"11076877845168992135"}},"outputId":"9ff8e146-7408-4e8c-8e1f-0c81a73fc814","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["##### test the data loader functions\n","# from dataloader.dataloader import get_scene, get_patch\n","# folder_TrainScenes = '/content/drive/My Drive/Colab/WaterMapping/TrainingData/TrainingScene/' \n","# folder_TrainTruths = '/content/drive/My Drive/Colab/WaterMapping/TrainingData/TrainingTruth/'\n","# PATCH_SIZE = 512\n","# BATCH_SIZE = 4\n","# BUFFER_SIZE = 200\n","# Scenes, Truths = get_scene(folder_TrainScenes, folder_TrainTruths, PATCH_SIZE=512)\n","# TrainSet = get_patch(Scenes, Truths, PATCH_SIZE=512, BATCH_SIZE=4, BUFFER_SIZE=200)\n","# TrainSet\n"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<ShuffleDataset shapes: ((None, 512, 512, 6), (None, 512, 512, 1)), types: (tf.float32, tf.float32)>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"LoYEePkT1oqE","executionInfo":{"status":"ok","timestamp":1602390068378,"user_tz":-480,"elapsed":17716,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"11076877845168992135"}}},"source":[""],"execution_count":3,"outputs":[]}]}